<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="LLINet" />
<meta property="og:description" content="Look, Listen and Infer" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/llinet/" />
<meta property="article:published_time" content="2020-07-02T20:49:26+08:00" />
<meta property="article:modified_time" content="2020-07-02T20:49:26+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LLINet"/>
<meta name="twitter:description" content="Look, Listen and Infer"/>


    <meta name="description" content="Look, Listen and Infer">
    <link rel="canonical" href="http://example.org/posts/llinet/">

    
    <title>LLINet &middot; Look, Listen and Infer</title>

    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <link href="http://example.org/css/style.css" rel="stylesheet">

    

    

    
  </head>
  <body>
    
      



<nav class="white" role="navigation">
    <div class="row max-width">
        <div class="col s12 l10 offset-l1">
            
            <a href="#" data-target="nav-mobile" class="sidenav-trigger black-text">
                <i class="material-icons">menu</i>
            </a>

            
            <ul id="nav-mobile" class="sidenav">
                
                
    
        
            
<li>
    <a class="black-text" href="/">Home</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/llinet">LLINet</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/dataset">Dataset</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/results">Results</a>
</li>

        
    
    

            </ul>

            
            <a href="/" class="brand-logo grey-text text-darken-3">Look, Listen and Infer</a>

            
            <div class="nav-wrapper">

                
                <ul class="right hide-on-med-and-down">
                    
                    
    
        
            
<li>
    <a class="black-text" href="/">Home</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/llinet">LLINet</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/dataset">Dataset</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/results">Results</a>
</li>

        
    
    

                </ul>

            </div>
        </div>
    </div>
</nav>
    

    

<article class="max-width">
    
    <section class="row">
        <div class="col s12 m10 offset-m1 l10 offset-l1">
            <h1>LLINet</h1>
        </div>
    </section>

    
    

    
    <section class="row">
        <div class="col s12 m8 offset-m2 l2 offset-l1">
            

<p class="article-meta">
    <div class="article-meta-container">
        <div class="article-meta-author-name"></div>
        <div class="article-meta-description">Look, Listen and Infer</div>
    </div>
    <span class="article-meta-published-at grey-text text-darken-1">Jul 2, 2020</span>
</p>
        </div>
        <div class="col s12 m8 offset-m2 l6" style = "text-align: justify; font-size: 1.25em;font-weight: 300;">
            

<h4 id="abstract">Abstract</h4>

<hr>

<p>In this work, for the first time, a <strong>Look, Listen and Infer Network</strong> (LLINet) is proposed to learn a zero-shot model that can infer the relations of visual scenes and sounds from novel categories never appeared before.</p>

<div align="center">
    <img src="source/pipeline.jpg" alt="pipeline" style="width:60%;" />
</div>

<p>LLINet is  mainly desired to qualify for two tasks, i.e., image-audio cross-modal retrieval and sound localization in each image. Towards this end, it is designed as a two-branch encoding network that builds a common space for images and audios.Besides, a cross-modal attention mechanism is proposed in LLINet to localize sound objects. To evaluate LLINet, a new data set, named INSTRUMENT-32CLASS, is collected in this work.</p>

<p>Besides zero-shot cross-modal retrieval and sound localization, a zero-shot image recognition task based on sounds is also conducted on this database. All experimental results on these tasks demonstrate the effectiveness of LLINet, indicating that zero-shot learning for visual scenes and sounds is feasible.</p>

<h4 id="framework">Framework</h4>

<hr>

<p>LLINet respectively utilizes an audio encoder and a visual encoder to embed input audios and images into a shared common space, sothat simple nearest neighbor methods can be used to perform thecross-modal retrieval task.</p>

<div align="center">
    <img src="source/modelframework1.jpg" alt="modelframework1" style="width:90%;"/>
</div>

<p><strong>Audio Encoder:</strong> The audio encoder consists of one convolution block, two residual connected blocks and a fully connected layer. The architecture of the convolution block is similar with the one in ResNet18. The last layer of the audio encoder is a single fully connected layer, mapping each audio feature into a 1024-d vector.</p>

<p><strong>Image Encoder :</strong>   ResNet-101, pre-trained on the ImageNet,  is adopted  as the image feature extractor. On top of it, two-layer linear layer is employed to map visual features into the common space.</p>

<p><strong>Attention module :</strong> The attention module consists of an attention layer and a transformer. Given an image-audio pair, the attention layer and the transformer are respectively used to project the tensor vector  extracted from the second block of the image encoder and the global audio feature into a common space, so that we can calculate similarity scores between each local position of image and the audio feature.</p>

        </div>
    </section>
</article>



    
      <footer class="page-footer grey lighten-5">
    <div class="row max-width">
        <div class="col s12 l10 offset-l1 clear-padding">
            <div class="row">
    
        
    

    
    
    <div class="col s12 l12">
        <h5 class="black-text">Data and codes</h5>
<p class="grey-text text-darken-4">The code of LLINet has been released on github, you can refer and download <a href="https://github.com/LLINet/LLINet">here</a>.</p>
<p class="grey-text text-darken-4">INSTRUMENTS-32CLASS, the dataset of LLINet, has been released on Google drive, you can download <a href="https://drive.google.com/file/d/1O193VHG5FAmt8XRLyB4bZWyWE-KQoAQn/view?usp=sharing">here</a>.</p>
    </div>

    
</div>


        </div>
    </div>
    <div class="footer-copyright">
        <div class="row max-width" style="width: 100%;">
            <div class="col s12 l10 offset-l1">
                <span class="grey-text text-darken-4"></span>
<div class="right">
    
</div>
            </div>
        </div>
    </div>
</footer>
    

    
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
    <script src="http://example.org/js/script.js"></script>
  </body>
</html>
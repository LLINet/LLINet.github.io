<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Results" />
<meta property="og:description" content="Results  The model is trained and evaluated on three tasks:
1. Zero-Shot Cross-Model Retrieval 2. Zero-Shot Sound Localization 3. Zero-Shot Recognition.  Zero-Shot Cross-Model Retrieval  Note that there are many excellent image-text cross-modal retrieval models in the literature. To compare with these image-text models, we replace their text encoders by our audio encoder. For a fair comparison, the input audios for all methods are represented by log Mel filter bank spectrograms as in our method." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/results/" />
<meta property="article:published_time" content="2020-07-02T21:23:46+08:00" />
<meta property="article:modified_time" content="2020-07-02T21:23:46+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Results"/>
<meta name="twitter:description" content="Results  The model is trained and evaluated on three tasks:
1. Zero-Shot Cross-Model Retrieval 2. Zero-Shot Sound Localization 3. Zero-Shot Recognition.  Zero-Shot Cross-Model Retrieval  Note that there are many excellent image-text cross-modal retrieval models in the literature. To compare with these image-text models, we replace their text encoders by our audio encoder. For a fair comparison, the input audios for all methods are represented by log Mel filter bank spectrograms as in our method."/>


    <meta name="description" content="">
    <link rel="canonical" href="http://example.org/posts/results/">

    
    <title>Results &middot; Look, Listen and Infer</title>

    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <link href="http://example.org/css/style.css" rel="stylesheet">

    

    

    
  </head>
  <body>
    
      



<nav class="white" role="navigation">
    <div class="row max-width">
        <div class="col s12 l10 offset-l1">
            
            <a href="#" data-target="nav-mobile" class="sidenav-trigger black-text">
                <i class="material-icons">menu</i>
            </a>

            
            <ul id="nav-mobile" class="sidenav">
                
                
    
        
            
<li>
    <a class="black-text" href="/">Home</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/llinet">LLINet</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/dataset">Dataset</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/results">Results</a>
</li>

        
    
    

            </ul>

            
            <a href="/" class="brand-logo grey-text text-darken-3">Look, Listen and Infer</a>

            
            <div class="nav-wrapper">

                
                <ul class="right hide-on-med-and-down">
                    
                    
    
        
            
<li>
    <a class="black-text" href="/">Home</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/llinet">LLINet</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/dataset">Dataset</a>
</li>

        
    
        
            
<li>
    <a class="black-text" href="/posts/results">Results</a>
</li>

        
    
    

                </ul>

            </div>
        </div>
    </div>
</nav>
    

    

<article class="max-width">
    
    <section class="row">
        <div class="col s12 m10 offset-m1 l10 offset-l1">
            <h1>Results</h1>
        </div>
    </section>

    
    

    
    <section class="row">
        <div class="col s12 m8 offset-m2 l2 offset-l1">
            

<p class="article-meta">
    <div class="article-meta-container">
        <div class="article-meta-author-name"></div>
        <div class="article-meta-description"></div>
    </div>
    <span class="article-meta-published-at grey-text text-darken-1">Jul 2, 2020</span>
</p>
        </div>
        <div class="col s12 m8 offset-m2 l6" style = "text-align: justify; font-size: 1.25em;font-weight: 300;">
            

<h3 id="results">Results</h3>

<hr>

<p>The model is trained and evaluated on three tasks:</p>

<pre><code>1. Zero-Shot Cross-Model Retrieval
2. Zero-Shot Sound Localization
3. Zero-Shot Recognition.
</code></pre>

<h5 id="zero-shot-cross-model-retrieval"><strong>Zero-Shot Cross-Model Retrieval</strong></h5>

<hr>

<p>Note that there are many excellent image-text cross-modal retrieval models in the literature. To compare with these image-text  models, we replace their text encoders by our audio encoder. For a fair comparison, the input audios for all methods are represented by log Mel filter bank spectrograms as in our method.</p>

<p><strong>Evaluation Metrics:</strong> Two-direction mAP, Two-direction R@1</p>

<p><strong>Result:</strong></p>

<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">mAP(I2A)</th>
<th align="center">R@1(I2A)</th>
<th align="center">mAP(A2I)</th>
<th align="center">R@1(A2I)</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">DAR</td>
<td align="center">13.3</td>
<td align="center">13.4</td>
<td align="center">13.6</td>
<td align="center">15.3</td>
</tr>

<tr>
<td align="center">ULSLVC</td>
<td align="center">13.4</td>
<td align="center">8.2</td>
<td align="center">20.1</td>
<td align="center">21.0</td>
</tr>

<tr>
<td align="center">SIR</td>
<td align="center">21.2</td>
<td align="center">20.9</td>
<td align="center">15.4</td>
<td align="center">12.8</td>
</tr>

<tr>
<td align="center">SCAN</td>
<td align="center">16.5</td>
<td align="center">16.7</td>
<td align="center">18.2</td>
<td align="center">15.0</td>
</tr>

<tr>
<td align="center">DSCMR</td>
<td align="center">14.8</td>
<td align="center">20.5</td>
<td align="center">18.1</td>
<td align="center">24.6</td>
</tr>

<tr>
<td align="center">TIMAM</td>
<td align="center">39.1</td>
<td align="center">28.5</td>
<td align="center">25.8</td>
<td align="center">25.0</td>
</tr>

<tr>
<td align="center">CMPM</td>
<td align="center">39.3</td>
<td align="center">28.9</td>
<td align="center">26.2</td>
<td align="center">25.8</td>
</tr>

<tr>
<td align="center">CME</td>
<td align="center">30.1</td>
<td align="center">25.7</td>
<td align="center">27.4</td>
<td align="center">30.8</td>
</tr>

<tr>
<td align="center"><strong>LLINet</strong></td>
<td align="center"><strong>49.3</strong></td>
<td align="center"><strong>41.1</strong></td>
<td align="center"><strong>31.2</strong></td>
<td align="center"><strong>38.3</strong></td>
</tr>
</tbody>
</table>

<p><strong>Visualization:</strong></p>

<div align="center">
    <img src="retrieval.jpg" alt="retrieval" style="width:80%;" />
</div>

<h5 id="zero-shot-sound-localization">Zero-Shot Sound Localization</h5>

<hr>

<p>Based on the attention module, the highlight region (with larger value) of the attention map can be taken as the related region of the sound source.</p>

<p><strong>Comparing model:</strong>  AVOL</p>

<p><strong>Ablation study:</strong></p>

<ol>
<li><strong>Baseline:</strong> Take out the attention module of LLINet, and train the model only with the matching loss</li>
<li><strong>Baseline+AM:</strong> Add introduced attention module but train the model only still with the matching loss</li>
<li><strong>LLINet:</strong> Based on Baseline+AM and optimized with both the matching loss and attention loss</li>
</ol>

<p><strong>Evaluation Metrics:</strong> mIoU, AUC</p>

<p><strong>Results:</strong></p>

<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">mIoU</th>
<th align="center">AUC</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">AVOL</td>
<td align="center">26.7</td>
<td align="center">27.1</td>
</tr>

<tr>
<td align="left">Baseline</td>
<td align="center">24.1</td>
<td align="center">24.6</td>
</tr>

<tr>
<td align="left">Baseline+AM</td>
<td align="center">32.3</td>
<td align="center">32.7</td>
</tr>

<tr>
<td align="left">LLINet</td>
<td align="center">36.8</td>
<td align="center">37.1</td>
</tr>
</tbody>
</table>

<p><strong>Visualization:</strong></p>

<ol>
<li><strong>Success plots:</strong></li>
</ol>

<div align="center">
    <img src="plot.jpg" alt="plot" style="width:52%;" />
</div align="center">

2. **Localization:**

<div align="center">
    <img src="localization.jpg" alt="localization" style="width:90%;"/>
</div>

<p><strong>Zero-Shot Recognition</strong></p>

<hr>

<p><strong>Prototype Vector:</strong> To perform ZSL, we obtain the prototype vector of a class by  averaging all audio features from this class, where audio features are extracted by the audio encoder of LLINet.</p>

<p><strong>GZSL:</strong> A classifier is trained for the seen classes. During the test process, this classifier is used to estimate whether an image is from seen or unseen classes, via comparing the confidence score with a threshold.</p>

<p><strong>Comparing models:</strong> Since this is the first work on zero-shot image recognition based on audio information, we compare LLINet with several representative ZSL methods. we utilize VGGish for comparing models to extract audio features, which is an audio classification model pre-trained on YouTube-100M.</p>

<p><strong>Evaluation metrics:</strong></p>

<ol>
<li>ZSL: accuracy</li>
<li>GZSL: tr, ts, hm</li>
</ol>

<p><strong>Results:</strong></p>

<table>
<thead>
<tr>
<th>Model</th>
<th>acc</th>
<th>tr</th>
<th>ts</th>
<th>hm</th>
</tr>
</thead>

<tbody>
<tr>
<td>DCN</td>
<td>29.8</td>
<td>48.0</td>
<td>26.0</td>
<td>33.7</td>
</tr>

<tr>
<td>LDE</td>
<td>51.7</td>
<td>61.7</td>
<td>39.9</td>
<td>48.5</td>
</tr>

<tr>
<td>Relation Net</td>
<td>55.4</td>
<td>60.8</td>
<td>40.0</td>
<td>48.0</td>
</tr>

<tr>
<td>SAE</td>
<td>57.7</td>
<td>63.7</td>
<td>38.7</td>
<td>48.1</td>
</tr>

<tr>
<td>LLINet</td>
<td>56.7</td>
<td>68.6</td>
<td>39.4</td>
<td>50.1</td>
</tr>

<tr>
<td><strong>LLINet(VGGish)</strong></td>
<td><strong>62.2</strong></td>
<td><strong>68.7</strong></td>
<td><strong>41.7</strong></td>
<td><strong>51.9</strong></td>
</tr>
</tbody>
</table>

        </div>
    </section>
</article>



    
      <footer class="page-footer grey lighten-5">
    <div class="row max-width">
        <div class="col s12 l10 offset-l1 clear-padding">
            <div class="row">
    
        
    

    
    
    <div class="col s12 l12">
        <h5 class="black-text">Data and codes</h5>
<p class="grey-text text-darken-4">The code of LLINet has been released on github, you can refer and download <a href="">here</a>.</p>
<p class="grey-text text-darken-4">INSTRUMENTS-32CLASS, the dataset of LLINet, has been released on github too, you can download <a href="">here</a>.</p>
    </div>

    
</div>


        </div>
    </div>
    <div class="footer-copyright">
        <div class="row max-width" style="width: 100%;">
            <div class="col s12 l10 offset-l1">
                <span class="grey-text text-darken-4"></span>
<div class="right">
    
</div>
            </div>
        </div>
    </div>
</footer>
    

    
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
    <script src="http://example.org/js/script.js"></script>
  </body>
</html>